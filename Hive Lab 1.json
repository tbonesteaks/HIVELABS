{"paragraphs":[{"title":"Exploring Hive","text":"%md\nThe overwhelming trend towards digital services, combined with cheap storage, has generated massive amounts of data that enterprises need to effectively gather, process, and analyze. Data analysis techniques from the data warehouse and high-performance computing communities are invaluable for many enterprises, however often times their cost or complexity of scale-up discourages the accumulation of data without an immediate need. As valuable knowledge may nevertheless be buried in this data, related scaled-up technologies have been developed. Examples include Google’s MapReduce, and the open-source implementation, Apache Hadoop.\n\nWriting MapReduce programs to analyze your Big Data can get complex. Apache Hive can help make querying your data much easier. Hive, first created at Facebook, is a data warehouse system for Hadoop that facilitates easy data summarization, ad-hoc queries, and the analysis of large datasets stored in Hadoop compatible file systems. Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL.","dateUpdated":"2016-11-18T17:47:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171114_-492276989","id":"20161101-205652_780108639","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The overwhelming trend towards digital services, combined with cheap storage, has generated massive amounts of data that enterprises need to effectively gather, process, and analyze. Data analysis techniques from the data warehouse and high-performance computing communities are invaluable for many enterprises, however often times their cost or complexity of scale-up discourages the accumulation of data without an immediate need. As valuable knowledge may nevertheless be buried in this data, related scaled-up technologies have been developed. Examples include Google’s MapReduce, and the open-source implementation, Apache Hadoop.</p>\n<p>Writing MapReduce programs to analyze your Big Data can get complex. Apache Hive can help make querying your data much easier. Hive, first created at Facebook, is a data warehouse system for Hadoop that facilitates easy data summarization, ad-hoc queries, and the analysis of large datasets stored in Hadoop compatible file systems. Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL.</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:47:59+0000","dateFinished":"2016-11-18T17:47:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:108","focus":true},{"title":"Lab 1 Introduction","text":"%md\nThis is a notebook based environment, called Zeppelin. It works with a set of interpreters in an interactive environment. \n\nYou will submit Apache Hive's SQL-like language known as \"HiveQL\" (or \"HQL\") through this interface. Results are returned back to this interface as well. \n\nWhat you are reading now is in a markdown paragraph, which is just an HTML-like, text-based, paragraph. You prefix each paragraph (or a note) with the interpreter you'd like it to run. In this case, we're using the %md (for markdown). You will use the JDBC interpreter to send your HiveQL to the Hive Server. To do this, the paragraphs will be prefixed with <b>%jdbc(hive)</b>  The JDBC interpeter can interact with a variety of JDBC data sources - in order for it to know you want to access a Hive data source, you must make sure the \"hive\" is in those parenthesis %jdbc(<b><u>hive</b></u>).\n\nYou will also see paragrahps that being with <b>%sh</b>. Those paragraphs run shell commands.","dateUpdated":"2016-11-18T17:56:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171115_-492661738","id":"20161109-194926_192026842","result":{"code":"SUCCESS","type":"HTML","msg":"<p>This is a notebook based environment, called Zeppelin. It works with a set of interpreters in an interactive environment.</p>\n<p>You will submit Apache Hive's SQL-like language known as &ldquo;HiveQL&rdquo; (or &ldquo;HQL&rdquo;) through this interface. Results are returned back to this interface as well.</p>\n<p>What you are reading now is in a markdown paragraph, which is just an HTML-like, text-based, paragraph. You prefix each paragraph (or a note) with the interpreter you'd like it to run. In this case, we're using the %md (for markdown). You will use the JDBC interpreter to send your HiveQL to the Hive Server. To do this, the paragraphs will be prefixed with <b>%jdbc(hive)</b>  The JDBC interpeter can interact with a variety of JDBC data sources - in order for it to know you want to access a Hive data source, you must make sure the &ldquo;hive&rdquo; is in those parenthesis %jdbc(<b><u>hive</b></u>).</p>\n<p>You will also see paragrahps that being with <b>%sh</b>. Those paragraphs run shell commands.</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:55:59+0000","dateFinished":"2016-11-18T17:55:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:109","focus":true},{"title":"Tasks","text":"%md\nYou are to go through this notebook and follow the instructions in either the markdown paragraphs, or the actual Hive paragraphs. You will run the paragraphs by clicking the \"Play\" button to the right of each paragraph. Running a markdown paragraph will just print the text out onto the results pane.","dateUpdated":"2016-11-18T17:48:06+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171115_-492661738","id":"20161109-204524_1386125688","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You are to go through this notebook and follow the instructions in either the markdown paragraphs, or the actual Hive paragraphs. You will run the paragraphs by clicking the &ldquo;Play&rdquo; button to the right of each paragraph. Running a markdown paragraph will just print the text out onto the results pane.</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:06+0000","dateFinished":"2016-11-18T17:48:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:110","focus":true},{"title":"Exploring the Hive environment","text":"%md\nIn this cloud environment, Hive is running in a remote container. Let’s navigate to the Hive home directory on the Linux file system within this Hive container and investigate the directories that Hive is comprised of.","dateUpdated":"2016-11-18T17:48:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171115_-492661738","id":"20161109-204127_942719256","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this cloud environment, Hive is running in a remote container. Let’s navigate to the Hive home directory on the Linux file system within this Hive container and investigate the directories that Hive is comprised of.</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:09+0000","dateFinished":"2016-11-18T17:48:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:111","focus":true},{"text":"%sh\nssh root@hive \"ls -l /opt/hive\"","dateUpdated":"2016-11-15T20:37:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171116_-494585483","id":"20161109-200741_568314914","dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-15T20:37:42+0000","dateFinished":"2016-11-15T20:37:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:112","errorMessage":""},{"text":"%md\nIn the above file and directory listing, you will notice the following directories of interest:\n\n•\tbin – executables to start/stop/configure/check status of hive, various scripts\n•\tconf – Hive environment, metastore, security, and log configuration files\n•\texamples – Hive examples\n•\thcatalog – Hcatalog files\n•\tjdbc –  Contains the hive-jdbc-2.1.0-standalone.jar file\n•\tlib – server’s JAR files\n•\tscripts – scripts for upgrading derby and MySQL metastores from one version of Hive to the next","dateUpdated":"2016-11-18T17:48:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171116_-494585483","id":"20161109-204729_1780232254","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the above file and directory listing, you will notice the following directories of interest:</p>\n<p>•   bin – executables to start/stop/configure/check status of hive, various scripts\n<br  />•   conf – Hive environment, metastore, security, and log configuration files\n<br  />•   examples – Hive examples\n<br  />•   hcatalog – Hcatalog files\n<br  />•   jdbc –  Contains the hive-jdbc-2.1.0-standalone.jar file\n<br  />•   lib – server’s JAR files\n<br  />•   scripts – scripts for upgrading derby and MySQL metastores from one version of Hive to the next</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:14+0000","dateFinished":"2016-11-18T17:48:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:113","focus":true},{"text":"%md\nIn the next paragraph you will list out the contents of Hive's /bin directory. Notice the entry named <b>beeline</b>. \nIf you were working on the command line and wanted to interact with Hive, Beeline would be a tool you would likely use. However, you are in the cloud, so you wont need to play with that!","dateUpdated":"2016-11-18T17:48:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171116_-494585483","id":"20161109-205325_130600862","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the next paragraph you will list out the contents of Hive's /bin directory. Notice the entry named <b>beeline</b>.\n<br  />If you were working on the command line and wanted to interact with Hive, Beeline would be a tool you would likely use. However, you are in the cloud, so you wont need to play with that!</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:18+0000","dateFinished":"2016-11-18T17:48:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:114","focus":true},{"text":"%sh\nssh root@hive \"ls -l /opt/hive/bin\"","dateUpdated":"2016-11-15T20:39:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171116_-494585483","id":"20161109-205307_134015993","dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-15T20:39:58+0000","dateFinished":"2016-11-15T20:39:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:115","errorMessage":""},{"text":"%md\nNow let's take a quick look at the Hadoop filesystem (HDFS) that is connected to our Hive container, to see what directories are currently on there.","dateUpdated":"2016-11-18T17:48:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171117_-494970231","id":"20161109-205720_849754921","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's take a quick look at the Hadoop filesystem (HDFS) that is connected to our Hive container, to see what directories are currently on there.</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:20+0000","dateFinished":"2016-11-18T17:48:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:116","focus":true},{"text":"%sh\nssh root@hive 'su - root bash -c \"hdfs dfs -ls /\"'","dateUpdated":"2016-11-15T20:40:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171117_-494970231","id":"20161109-202814_1876067554","dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-15T20:40:31+0000","dateFinished":"2016-11-15T20:40:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:117","errorMessage":""},{"text":"%md\nExcellent - HDFS looks quite clean. It wont be quite as clean, when we start adding Hive tables!\n\nOn HDFS there is a /user/hive/warehouse directory. If you'd like to confirm this, modify the above hdfs list command to the following:\n      ssh root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive\"'\n\nNext, let's run some actual HiveQL, just to prove it works. In the next paragraph, you will tell the JDBC interpeter to send the <b>SHOW DATABASES</b> command to your Hive instance.","dateUpdated":"2016-11-18T17:48:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171117_-494970231","id":"20161109-205825_281049148","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Excellent - HDFS looks quite clean. It wont be quite as clean, when we start adding Hive tables!</p>\n<p>On HDFS there is a /user/hive/warehouse directory. If you'd like to confirm this, modify the above hdfs list command to the following:</p>\n<pre><code>  ssh root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive\"'\n</code></pre>\n<p>Next, let's run some actual HiveQL, just to prove it works. In the next paragraph, you will tell the JDBC interpeter to send the <b>SHOW DATABASES</b> command to your Hive instance.</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:23+0000","dateFinished":"2016-11-18T17:48:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:118","focus":true},{"text":"%jdbc(hive)\nSHOW DATABASES","dateUpdated":"2016-11-15T20:40:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"database_name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"database_name","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171117_-494970231","id":"20161101-205912_733800417","dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-15T20:40:49+0000","dateFinished":"2016-11-15T20:40:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:119","errorMessage":""},{"text":"%md\nNotice that the only database returned in the above command, is the <b>default</b> database. \n\nThe default database is the database that automatically comes with Hive when it is first installed. You will of course be creating new databases in the upcoming labs!","dateUpdated":"2016-11-18T17:48:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171118_-493815985","id":"20161111-151250_334089312","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notice that the only database returned in the above command, is the <b>default</b> database.</p>\n<p>The default database is the database that automatically comes with Hive when it is first installed. You will of course be creating new databases in the upcoming labs!</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:28+0000","dateFinished":"2016-11-18T17:48:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:120","focus":true},{"text":"%md\nFinally, you can take a look at all of the Hive configs that are currently set, but running the <b>set</b> command. Run the command below and take a little time to explore the config values that are returned.","dateUpdated":"2016-11-18T17:48:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171118_-493815985","id":"20161114-154838_129759610","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Finally, you can take a look at all of the Hive configs that are currently set, but running the <b>set</b> command. Run the command below and take a little time to explore the config values that are returned.</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:30+0000","dateFinished":"2016-11-18T17:48:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:121","focus":true},{"text":"%jdbc(hive)\nset","dateUpdated":"2016-11-15T20:37:46+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"set","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"set","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171118_-493815985","id":"20161111-190336_328161733","dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-15T20:37:46+0000","dateFinished":"2016-11-15T20:37:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:122","errorMessage":""},{"text":"%md\n\nCongratulations! You now know how to navigate to the Hive directories and understand the contents of those directories. You also know how to run basic HDFS and Hive commands.\nYou may move on to the next Unit. ","dateUpdated":"2016-11-18T17:48:50+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242171127_-484966760","id":"20161114-154534_961742200","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Congratulations! You now know how to navigate to the Hive directories and understand the contents of those directories. You also know how to run basic HDFS and Hive commands.\n<br  />You may move on to the next Unit.</p>\n"},"dateCreated":"2016-11-15T20:36:11+0000","dateStarted":"2016-11-18T17:48:48+0000","dateFinished":"2016-11-18T17:48:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:123","focus":true},{"text":"","dateUpdated":"2016-11-16T18:24:10+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479320636071_1210888748","id":"20161116-182356_637753985","dateCreated":"2016-11-16T18:23:56+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:124","errorMessage":""}],"name":"Hive Lab 1","id":"2C1DJEPAC","angularObjects":{"2C1FSWPV5:shared_process":[],"2C2ANMC3H:shared_process":[],"2C27C2CAW:shared_process":[],"2C1EQ4DYT:shared_process":[],"2C2D85D6Q:shared_process":[],"2BYMYHYE7:shared_process":[],"2BZJ3HWCJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}