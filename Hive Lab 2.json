{"paragraphs":[{"title":"Working with Hive DDL","text":"%md\nBefore we can begin working with and analyzing data in Hive we must first use Hive’s data manipulation language. Using Hive DML will enable us to create databases, tables, partitions and more which we can later load with data that can be queried and manipulated.","dateUpdated":"2016-11-18T17:58:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177984_1787744993","id":"20161101-210008_964712558","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Before we can begin working with and analyzing data in Hive we must first use Hive’s data manipulation language. Using Hive DML will enable us to create databases, tables, partitions and more which we can later load with data that can be queried and manipulated.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:276","dateFinished":"2016-11-18T17:58:47+0000","dateStarted":"2016-11-18T17:58:47+0000","focus":true},{"title":"Lab 2 Tasks","text":"%md\nYou are to go through this notebook and follow the instructions in either the markdown paragraphs, or the actual Hive paragraphs. You will run the paragraphs by clicking the “Play” button to the right of each paragraph. Running a markdown paragraph will just print the text out onto the results pane.","dateUpdated":"2016-11-18T17:59:06+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177984_1787744993","id":"20161111-153517_1573235838","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You are to go through this notebook and follow the instructions in either the markdown paragraphs, or the actual Hive paragraphs. You will run the paragraphs by clicking the “Play” button to the right of each paragraph. Running a markdown paragraph will just print the text out onto the results pane.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:277","dateFinished":"2016-11-18T17:59:07+0000","dateStarted":"2016-11-18T17:59:07+0000","focus":true},{"text":"%md\nBefore you begin to create new Hive databases within the Hive warehouse, first show the databases in the system by running the <b>SHOW DATABASES</b> command.","dateUpdated":"2016-11-18T17:59:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177984_1787744993","id":"20161111-153932_1569239589","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Before you begin to create new Hive databases within the Hive warehouse, first show the databases in the system by running the <b>SHOW DATABASES</b> command.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:278","dateFinished":"2016-11-18T17:59:09+0000","dateStarted":"2016-11-18T17:59:09+0000","focus":true},{"text":"%jdbc(hive)\nSHOW DATABASES","dateUpdated":"2016-11-15T20:43:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177985_1787360244","id":"20161111-153921_859418621","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:43:52+0000","dateFinished":"2016-11-15T20:43:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:279","errorMessage":""},{"title":"Working with Databases in Hive","text":"%md\nIf we neglect to create a new database in Hive, then the “default” database will be used. Let’s create a new database and work with it. In this exercise we will create two databases in the Hive system. \nOne of these new databses will be used for future exercises. The other will be deleted.\n\nThe first database you create will be called <b>testDB</b>.","dateUpdated":"2016-11-18T17:59:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177985_1787360244","id":"20161111-154100_1512772387","result":{"code":"SUCCESS","type":"HTML","msg":"<p>If we neglect to create a new database in Hive, then the “default” database will be used. Let’s create a new database and work with it. In this exercise we will create two databases in the Hive system.\n<br  />One of these new databses will be used for future exercises. The other will be deleted.</p>\n<p>The first database you create will be called <b>testDB</b>.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:280","dateFinished":"2016-11-18T17:59:14+0000","dateStarted":"2016-11-18T17:59:14+0000","focus":true},{"text":"%jdbc(hive)\nCREATE DATABASE IF NOT EXISTS testDB","dateUpdated":"2016-11-15T20:46:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Update Count","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"Update Count","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177985_1787360244","id":"20161101-210252_1499733943","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:46:10+0000","dateFinished":"2016-11-15T20:46:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:281","errorMessage":""},{"text":"%jdbc(hive)\nSHOW DATABASES","dateUpdated":"2016-11-15T20:46:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177985_1787360244","id":"20161101-212900_1987060191","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:46:15+0000","dateFinished":"2016-11-15T20:46:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:282","errorMessage":""},{"text":"%md\nAbove, you run the SHOW DATABASES command to confirm that the new database was in fact created. Notice that Hive converted the “testDB” text to lowercase when it created the new database in the system.\n\nNow that you have created a new database named testdb, let’s describe it.","dateUpdated":"2016-11-18T17:59:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177986_1788514491","id":"20161111-154226_1532117178","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Above, you run the SHOW DATABASES command to confirm that the new database was in fact created. Notice that Hive converted the “testDB” text to lowercase when it created the new database in the system.</p>\n<p>Now that you have created a new database named testdb, let’s describe it.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:283","dateFinished":"2016-11-18T17:59:19+0000","dateStarted":"2016-11-18T17:59:19+0000","focus":true},{"text":"%jdbc(hive)\nDESCRIBE DATABASE testdb","dateUpdated":"2016-11-15T20:46:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"db_name","index":0,"aggr":"sum"}],"values":[{"name":"comment","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"db_name","index":0,"aggr":"sum"},"yAxis":{"name":"comment","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177986_1788514491","id":"20161101-213258_1064731328","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:46:21+0000","dateFinished":"2016-11-15T20:46:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:284","errorMessage":""},{"text":"%md\nThe DESCRIBE DATABASE command above returns some information about the database, including the location of testdb on HDFS. \nNotice that the testdb.db schema is actually a directory that was created on HDFS within the /user/hive/warehouse directory.\n\nLet's check HDFS and confirm that the testdb.db directory was created.","dateUpdated":"2016-11-18T17:59:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177986_1788514491","id":"20161111-154434_1940129289","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The DESCRIBE DATABASE command above returns some information about the database, including the location of testdb on HDFS.\n<br  />Notice that the testdb.db schema is actually a directory that was created on HDFS within the /user/hive/warehouse directory.</p>\n<p>Let's check HDFS and confirm that the testdb.db directory was created.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:285","dateFinished":"2016-11-18T17:59:24+0000","dateStarted":"2016-11-18T17:59:24+0000","focus":true},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/\"'","dateUpdated":"2016-11-15T20:46:41+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177986_1788514491","id":"20161101-213333_724799562","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:46:41+0000","dateFinished":"2016-11-15T20:46:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:286","errorMessage":""},{"text":"%md\nAdd some information to the DBPROPERTIES metadata for the testdb database. You can do this by using the <b>ALTER DATABASE</b> syntax.","dateUpdated":"2016-11-18T17:59:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177986_1788514491","id":"20161114-155911_170605847","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Add some information to the DBPROPERTIES metadata for the testdb database. You can do this by using the <b>ALTER DATABASE</b> syntax.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:287","dateFinished":"2016-11-18T17:59:28+0000","dateStarted":"2016-11-18T17:59:28+0000","focus":true},{"text":"%jdbc(hive)\nALTER DATABASE testdb SET DBPROPERTIES ('creator' = 'bigdatarockstar')","dateUpdated":"2016-11-15T20:46:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Update Count","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"Update Count","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177987_1788129742","id":"20161101-213406_352966946","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:46:49+0000","dateFinished":"2016-11-15T20:46:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:288","errorMessage":""},{"text":"%md\nLet’s view the extended details of our testdb database.","dateUpdated":"2016-11-18T17:59:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177987_1788129742","id":"20161114-160103_648787207","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let’s view the extended details of our testdb database.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289","dateFinished":"2016-11-18T17:59:32+0000","dateStarted":"2016-11-18T17:59:32+0000","focus":true},{"text":"%jdbc(hive)\nDESCRIBE DATABASE EXTENDED testdb","dateUpdated":"2016-11-15T20:46:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177987_1788129742","id":"20161101-213445_1517256942","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:46:55+0000","dateFinished":"2016-11-15T20:46:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:290","errorMessage":""},{"text":"%md\nNotice the updated database properties above.\n\nNow go ahead and delete the testdb database.\n","dateUpdated":"2016-11-18T17:59:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177987_1788129742","id":"20161114-160148_330801812","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notice the updated database properties above.</p>\n<p>Now go ahead and delete the testdb database.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:291","dateFinished":"2016-11-18T17:59:36+0000","dateStarted":"2016-11-18T17:59:36+0000","focus":true},{"text":"%jdbc(hive)\nDROP DATABASE testdb CASCADE","dateUpdated":"2016-11-15T20:47:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177987_1788129742","id":"20161101-213532_2135568473","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:47:03+0000","dateFinished":"2016-11-15T20:47:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:292","errorMessage":""},{"text":"%md\nNotice the CASCADE keyword. That is optional. Using it will cause Hive to delete all the tables in your database (if there are any) before dropping the database. \nIf you try to delete a database that has tables without the CASCADE keyword, Hive won’t let you.\n\nConfirm that testdb is no longer in the Hive metastore catalog.","dateUpdated":"2016-11-18T17:59:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177988_1786205997","id":"20161114-160919_814533075","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notice the CASCADE keyword. That is optional. Using it will cause Hive to delete all the tables in your database (if there are any) before dropping the database.\n<br  />If you try to delete a database that has tables without the CASCADE keyword, Hive won’t let you.</p>\n<p>Confirm that testdb is no longer in the Hive metastore catalog.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:293","dateFinished":"2016-11-18T17:59:40+0000","dateStarted":"2016-11-18T17:59:40+0000","focus":true},{"text":"%jdbc(hive)\nSHOW DATABASES","dateUpdated":"2016-11-15T20:47:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177988_1786205997","id":"20161101-213549_1593309947","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:47:08+0000","dateFinished":"2016-11-15T20:47:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:294","errorMessage":""},{"text":"%md\nAbove, you can see that the testdb database was dropped and is no longer listed as a database within Hive.\n\nNow we are going to create a database that will house the tables we will use for many of the exercises in this course. This database will be called “computersalesdb”.","dateUpdated":"2016-11-18T17:59:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177988_1786205997","id":"20161114-160321_722928379","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Above, you can see that the testdb database was dropped and is no longer listed as a database within Hive.</p>\n<p>Now we are going to create a database that will house the tables we will use for many of the exercises in this course. This database will be called “computersalesdb”.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:295","dateFinished":"2016-11-18T17:59:44+0000","dateStarted":"2016-11-18T17:59:44+0000","focus":true},{"text":"%jdbc(hive)\nCREATE DATABASE IF NOT EXISTS computersalesdb","dateUpdated":"2016-11-15T20:47:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177988_1786205997","id":"20161101-213654_1005036809","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:47:14+0000","dateFinished":"2016-11-15T20:47:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:296","errorMessage":""},{"text":"%md\nVerify the DB was created. Note the location of the database directory in HDFS.","dateUpdated":"2016-11-18T17:59:47+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177988_1786205997","id":"20161114-162026_458424998","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Verify the DB was created. Note the location of the database directory in HDFS.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:297","dateFinished":"2016-11-18T17:59:47+0000","dateStarted":"2016-11-18T17:59:47+0000","focus":true},{"text":"%jdbc(hive)\nDESCRIBE DATABASE computersalesdb","dateUpdated":"2016-11-15T20:47:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177989_1785821248","id":"20161101-213707_1909184204","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:47:20+0000","dateFinished":"2016-11-15T20:47:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:298","errorMessage":""},{"text":"%md\nWe can see that the computersalesdb does in fact exist and the new directory was created on HDFS in the /user/hive/warehouse/computersalesdb.db directory.\n\nCheck HDFS and verify that the new directory for this database was created.","dateUpdated":"2016-11-18T17:59:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177989_1785821248","id":"20161114-162054_328903724","result":{"code":"SUCCESS","type":"HTML","msg":"<p>We can see that the computersalesdb does in fact exist and the new directory was created on HDFS in the /user/hive/warehouse/computersalesdb.db directory.</p>\n<p>Check HDFS and verify that the new directory for this database was created.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:299","dateFinished":"2016-11-18T17:59:51+0000","dateStarted":"2016-11-18T17:59:51+0000","focus":true},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/\"'","dateUpdated":"2016-11-15T20:47:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177989_1785821248","id":"20161101-213717_1709759024","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:47:29+0000","dateFinished":"2016-11-15T20:47:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:300","errorMessage":""},{"title":"Exploring the Sample Datasets","text":"%md\nBefore we begin creating tables in our new database it is important to understand what data is in our sample files and how that data is structured.\n\nFirst you will create a directory, chmod it to 777, and import the sample data files. Then you will analyze the files.\n\nTo import the data files, use the <b>wget</b> command to download them. Place the files in a newly created directory in /tmp/hive_data on the hive container's Linux filesystem. There are three csv data files that you will import below.\n","dateUpdated":"2016-11-18T17:59:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177990_1786975495","id":"20161101-230956_796591254","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Before we begin creating tables in our new database it is important to understand what data is in our sample files and how that data is structured.</p>\n<p>First you will create a directory, chmod it to 777, and import the sample data files. Then you will analyze the files.</p>\n<p>To import the data files, use the <b>wget</b> command to download them. Place the files in a newly created directory in /tmp/hive_data on the hive container's Linux filesystem. There are three csv data files that you will import below.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T17:59:57+0000","dateFinished":"2016-11-18T17:59:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:301","focus":true},{"text":"%sh\nssh root@hive 'su - root bash -c \"mkdir /tmp/hive_data\"'","dateUpdated":"2016-11-15T20:48:06+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177990_1786975495","id":"20161114-190753_90071577","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:48:06+0000","dateFinished":"2016-11-15T20:48:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:302","errorMessage":""},{"text":"%md\nUse the <b>ls</b> command to check the /tmp directory on the hive container to ensure the /tmp/hive_data directory was created.","dateUpdated":"2016-11-18T18:00:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177990_1786975495","id":"20161114-191240_6516557","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Use the <b>ls</b> command to check the /tmp directory on the hive container to ensure the /tmp/hive_data directory was created.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T18:00:01+0000","dateFinished":"2016-11-18T18:00:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:303","focus":true},{"text":"%sh\nssh root@hive \"ls -l /tmp\"","dateUpdated":"2016-11-15T20:48:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177990_1786975495","id":"20161114-191108_1081332914","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:48:27+0000","dateFinished":"2016-11-15T20:48:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:304","errorMessage":""},{"text":"%md\nUse the <b>chmod</b> command to set the permission for /tmp/hive_data to 777 (full read/write permission).","dateUpdated":"2016-11-18T18:00:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177990_1786975495","id":"20161114-191320_1408449481","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Use the <b>chmod</b> command to set the permission for /tmp/hive_data to 777 (full read/write permission).</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:305","dateFinished":"2016-11-18T18:00:05+0000","dateStarted":"2016-11-18T18:00:05+0000","focus":true},{"text":"%sh\nssh root@hive 'su - root bash -c \"chmod -R 777 /tmp/hive_data\"'","dateUpdated":"2016-11-15T20:48:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177990_1786975495","id":"20161114-191207_572530968","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:48:45+0000","dateFinished":"2016-11-15T20:48:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:306","errorMessage":""},{"text":"%md\nUse <b>wget</b> to download the three csv files to /tmp/hive_data on the hive container.","dateUpdated":"2016-11-18T18:00:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177991_1786590746","id":"20161114-193210_1703112088","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Use <b>wget</b> to download the three csv files to /tmp/hive_data on the hive container.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T18:00:08+0000","dateFinished":"2016-11-18T18:00:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:307","focus":true},{"text":"%sh\nssh root@hive 'su - root bash -c \"wget --quiet --output-document /tmp/hive_data/Customer.csv https://ibm.box.com/shared/static/fjcm8worqj0dki7n0g36nml5t7v8o8lu.csv\"'\nssh root@hive 'su - root bash -c \"wget --quiet --output-document /tmp/hive_data/Sales.csv https://ibm.box.com/shared/static/xzebk7ulqfep3lzuc57jsc2vboemlatb.csv\"'\nssh root@hive 'su - root bash -c \"wget --quiet --output-document /tmp/hive_data/Product.csv https://ibm.box.com/shared/static/kjcvfgcgnnvcoya7e8i7gdp0yircxodc.csv\"'\n","dateUpdated":"2016-11-18T17:57:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177991_1786590746","id":"20161114-190231_243127596","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T17:57:41+0000","dateFinished":"2016-11-18T17:57:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:308","errorMessage":"","focus":true},{"text":"%md\nCheck /tmp/hive_data to ensure the files were obtained.","dateUpdated":"2016-11-18T18:00:12+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177991_1786590746","id":"20161114-193325_1541204224","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Check /tmp/hive_data to ensure the files were obtained.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:309","dateFinished":"2016-11-18T18:00:13+0000","dateStarted":"2016-11-18T18:00:13+0000","focus":true},{"text":"%sh\nssh root@hive \"ls -l /tmp/hive_data\"","dateUpdated":"2016-11-18T17:57:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177991_1786590746","id":"20161114-193305_602766376","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T17:57:54+0000","dateFinished":"2016-11-18T17:57:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:310","errorMessage":"","focus":true},{"title":"Sample Data Descriptions","text":"%md\nNow you will analyze each data file. Before we do that, here is the metadata for each file.\n\nOur sample data is from a fictitious computer retailer. The company sells computer parts and generally serves a single State in the country.\n\n<h3>Customer.csv:</h3>\n\tPurpose: Hold customer records.\n\t\n\tColumns: \n\tFNAME\t                    Customer’s First Name\n\tLNAME\t                    Customer’s Last Name\n\tSTATUS\t                    Active or Inactive status\n\tTELNO\t                    Telephone #\tCustomer’s unique ID\n\tCUSTOMER_ID\tCITY|ZIP        City and Zip code separated by the “|” character.\n\t\n\t\n<h3>Product.csv:</h3>\n\tPurpose: Hold product records.\n\t\n\tColumns:\n\tPROD_NAME\t                Name of product\n\tDESCRIPTION\t                Description of computer product\n\tCATEGORY                    Category product belongs to\t\n\tQTY_ON_HAND\t                Quantity of product in warehouse\n\tPROD_NUM                    Unique product number\t\n\tPACKAGED_WITH               Colon separated list of things that come in package with product.\n\t\t\t\t\t\n\n<h3>Sales.csv:</h3>\n\tPurpose: Holds all historical sales records. Company updates once a month. \n\t\n\tColumns:\n\tCUST_ID\t                    ID of customer who made purchase\n\tPROD_NUM                    ID of product that was purchased\n\tQTY                         QTY purchased\n\tDATE                        Date of sale\n\tSALES_ID                    Unique sale ID \n\t\t\t\t\n\n\t\t\t\t\n","dateUpdated":"2016-11-18T18:00:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177992_1784667002","id":"20161114-193655_1094242984","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now you will analyze each data file. Before we do that, here is the metadata for each file.</p>\n<p>Our sample data is from a fictitious computer retailer. The company sells computer parts and generally serves a single State in the country.</p>\n<h3>Customer.csv:</h3>\n<pre><code>Purpose: Hold customer records.\n\nColumns: \nFNAME                       Customer’s First Name\nLNAME                       Customer’s Last Name\nSTATUS                      Active or Inactive status\nTELNO                       Telephone # Customer’s unique ID\nCUSTOMER_ID CITY|ZIP        City and Zip code separated by the “|” character.\n</code></pre>\n<h3>Product.csv:</h3>\n<pre><code>Purpose: Hold product records.\n\nColumns:\nPROD_NAME                   Name of product\nDESCRIPTION                 Description of computer product\nCATEGORY                    Category product belongs to \nQTY_ON_HAND                 Quantity of product in warehouse\nPROD_NUM                    Unique product number   \nPACKAGED_WITH               Colon separated list of things that come in package with product.\n</code></pre>\n<h3>Sales.csv:</h3>\n<pre><code>Purpose: Holds all historical sales records. Company updates once a month. \n\nColumns:\nCUST_ID                     ID of customer who made purchase\nPROD_NUM                    ID of product that was purchased\nQTY                         QTY purchased\nDATE                        Date of sale\nSALES_ID                    Unique sale ID\n</code></pre>\n"},"dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T18:00:17+0000","dateFinished":"2016-11-18T18:00:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:311","focus":true},{"text":"%md\nUse the Linux <b>cat</b> command to explore the contents of the data files.","dateUpdated":"2016-11-18T18:00:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177992_1784667002","id":"20161114-194447_1200078669","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Use the Linux <b>cat</b> command to explore the contents of the data files.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:312","dateFinished":"2016-11-18T18:00:22+0000","dateStarted":"2016-11-18T18:00:22+0000","focus":true},{"title":"Customer.csv","text":"%sh\nssh root@hive 'su - root bash -c \"cat /tmp/hive_data/Customer.csv\"'","dateUpdated":"2016-11-18T17:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177992_1784667002","id":"20161114-194521_1763228059","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T17:58:02+0000","dateFinished":"2016-11-18T17:58:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:313","errorMessage":"","focus":true},{"title":"Product.csv","text":"%sh\nssh root@hive 'su - root bash -c \"cat /tmp/hive_data/Product.csv\"'","dateUpdated":"2016-11-18T17:58:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177992_1784667002","id":"20161114-194658_612211889","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T17:58:09+0000","dateFinished":"2016-11-18T17:58:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:314","errorMessage":"","focus":true},{"title":"Sales.csv","text":"%sh\nssh root@hive 'su - root bash -c \"cat /tmp/hive_data/Sales.csv\"'","dateUpdated":"2016-11-18T17:58:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177992_1784667002","id":"20161114-194732_695759713","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-18T17:58:14+0000","dateFinished":"2016-11-18T17:58:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:315","errorMessage":"","focus":true},{"title":"Tables in Hive","text":"%md\nNow let's start working with Hive tables.","dateUpdated":"2016-11-18T18:00:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177993_1784282253","id":"20161114-194855_231140922","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's start working with Hive tables.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:316","dateFinished":"2016-11-18T18:00:29+0000","dateStarted":"2016-11-18T18:00:29+0000","focus":true},{"title":"Managed Non-Partitioned Tables","text":"%md\nThe first table we will create in Hive is the products table. This table will be fully managed by Hive and will not contain any partitions.\n\nNote the data types we have assigned to the different columns. The packaged_with column is of special interest – it is designated as an Array of Strings. The array will hold data that is separated by the colon “:” character - e.g. satacable:manual. We also tell Hive that the columns in our rows are delimited by commas “,”. The last line tells Hive that our data file is a plain text file.","dateUpdated":"2016-11-18T18:00:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177993_1784282253","id":"20161114-203120_1605044666","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The first table we will create in Hive is the products table. This table will be fully managed by Hive and will not contain any partitions.</p>\n<p>Note the data types we have assigned to the different columns. The packaged_with column is of special interest – it is designated as an Array of Strings. The array will hold data that is separated by the colon “:” character - e.g. satacable:manual. We also tell Hive that the columns in our rows are delimited by commas “,”. The last line tells Hive that our data file is a plain text file.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:317","dateFinished":"2016-11-18T18:00:32+0000","dateStarted":"2016-11-18T18:00:32+0000","focus":true},{"text":"%jdbc(hive)\nCREATE TABLE computersalesdb.products \n(\n  \t\tprod_name      STRING,\n \t \tdescription    STRING,\n  \t\tcategory       STRING,\n  \t\tqty_on_hand    INT,\n  \t\tprod_num       STRING,\n  \t\tpackaged_with  ARRAY<STRING>\n)\nROW FORMAT DELIMITED\n   \t\tFIELDS TERMINATED BY ','\n  \t \tCOLLECTION ITEMS TERMINATED BY ':'\nSTORED AS TEXTFILE\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\")","dateUpdated":"2016-11-15T20:50:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177993_1784282253","id":"20161101-213803_677297437","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:50:52+0000","dateFinished":"2016-11-15T20:50:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:318","errorMessage":""},{"text":"%jdbc(hive)\nSHOW TABLES IN computersalesdb","dateUpdated":"2016-11-15T20:50:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177994_1785436499","id":"20161101-213823_877134491","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:50:58+0000","dateFinished":"2016-11-15T20:50:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:319","errorMessage":""},{"text":"%md\nYou can see that only one table exists in our database and it is the new products table we just created.\n\nNow add a note to the TBLPROPERTIES for our new products table.","dateUpdated":"2016-11-18T18:00:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177994_1785436499","id":"20161114-203322_282111114","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You can see that only one table exists in our database and it is the new products table we just created.</p>\n<p>Now add a note to the TBLPROPERTIES for our new products table.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:320","dateFinished":"2016-11-18T18:00:37+0000","dateStarted":"2016-11-18T18:00:37+0000","focus":true},{"text":"%jdbc(hive)\nALTER TABLE computersalesdb.products SET TBLPROPERTIES ('details' = 'This table holds products')","dateUpdated":"2016-11-15T20:51:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177994_1785436499","id":"20161101-213902_1203558472","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:51:19+0000","dateFinished":"2016-11-15T20:51:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:321","errorMessage":""},{"text":"%md\nList the extended details of the products table.","dateUpdated":"2016-11-18T18:00:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177994_1785436499","id":"20161114-203425_155666405","result":{"code":"SUCCESS","type":"HTML","msg":"<p>List the extended details of the products table.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:322","dateFinished":"2016-11-18T18:00:42+0000","dateStarted":"2016-11-18T18:00:42+0000","focus":true},{"text":"%jdbc(hive)\nDESCRIBE EXTENDED computersalesdb.products","dateUpdated":"2016-11-15T20:51:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177995_1785051751","id":"20161101-214049_1513364090","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:51:36+0000","dateFinished":"2016-11-15T20:51:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:323","errorMessage":""},{"text":"%md\nThat is a lot of details! Notice there is some interesting info including the location of this table within HDFS: /user/hive/warehouse/computersalesdb.db/products\n\nLet’s verify that the products directory was created on HDFS in the location listed above. Run the HDFS ls command from within a Linux console. First list the contents of the database directory, then list the contents of the products table directory.","dateUpdated":"2016-11-18T18:00:47+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177995_1785051751","id":"20161114-203504_685150966","result":{"code":"SUCCESS","type":"HTML","msg":"<p>That is a lot of details! Notice there is some interesting info including the location of this table within HDFS: /user/hive/warehouse/computersalesdb.db/products</p>\n<p>Let’s verify that the products directory was created on HDFS in the location listed above. Run the HDFS ls command from within a Linux console. First list the contents of the database directory, then list the contents of the products table directory.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:324","dateFinished":"2016-11-18T18:00:47+0000","dateStarted":"2016-11-18T18:00:47+0000","focus":true},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db\"'","dateUpdated":"2016-11-15T20:51:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177995_1785051751","id":"20161114-203721_1251245988","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:51:58+0000","dateFinished":"2016-11-15T20:52:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:325","errorMessage":""},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db/products\"'","dateUpdated":"2016-11-15T20:52:12+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177995_1785051751","id":"20161101-231954_886902784","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:52:12+0000","dateFinished":"2016-11-15T20:52:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:326","errorMessage":""},{"text":"%md\nThe first <b>hdfs ls</b> command above confirms that there is in fact a products table directory on HDFS. The second command shows that there are no files within the products directory yet. \nThis directory will be empty until we load data into the products table in a later exercise.","dateUpdated":"2016-11-18T18:00:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177996_1783128006","id":"20161114-203829_835625165","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The first <b>hdfs ls</b> command above confirms that there is in fact a products table directory on HDFS. The second command shows that there are no files within the products directory yet.\n<br  />This directory will be empty until we load data into the products table in a later exercise.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:327","dateFinished":"2016-11-18T18:00:52+0000","dateStarted":"2016-11-18T18:00:52+0000","focus":true},{"text":"%md\nImagine that our fictitious computer company adds sales data to a “sales_staging” table at the end of each month. \nFrom this sales_staging table they then move the data they want to analyze into a partitioned “sales” table. The partitioned sales table is the one they actually use for their analysis.\n\nNow that you know how to create tables, you will create one more managed non-partitioned table called  “sales_staging”. This table will hold ALL of the sales data from the sales.csv file. In later exercises you will actually split this sales_staging data into a partitioned table called “sales”.\n\nYou will now create the new sales_staging table in Hive.\n","dateUpdated":"2016-11-18T18:00:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177996_1783128006","id":"20161114-204027_1513987798","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Imagine that our fictitious computer company adds sales data to a “sales_staging” table at the end of each month.\n<br  />From this sales_staging table they then move the data they want to analyze into a partitioned “sales” table. The partitioned sales table is the one they actually use for their analysis.</p>\n<p>Now that you know how to create tables, you will create one more managed non-partitioned table called  “sales_staging”. This table will hold ALL of the sales data from the sales.csv file. In later exercises you will actually split this sales_staging data into a partitioned table called “sales”.</p>\n<p>You will now create the new sales_staging table in Hive.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:328","dateFinished":"2016-11-18T18:00:55+0000","dateStarted":"2016-11-18T18:00:55+0000","focus":true},{"text":"%jdbc(hive)\nCREATE TABLE computersalesdb.sales_staging\n (\t\n  \t\tcust_id    STRING,\n \t \tprod_num   STRING,\n  \t\tqty        INT,\n  \t\tsale_date  DATE,\n  \t\tsales_id   STRING\n )\n COMMENT 'Staging table for sales data'\n ROW FORMAT DELIMITED\n   \t\tFIELDS TERMINATED BY ','\n STORED AS TEXTFILE\n TBLPROPERTIES(\"skip.header.line.count\"=\"1\")\n","dateUpdated":"2016-11-15T20:52:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177996_1783128006","id":"20161101-232019_1658704024","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:52:23+0000","dateFinished":"2016-11-15T20:52:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:329","errorMessage":""},{"text":"%md\nYou can now assume that the new sales_staging table directory is on HDFS in the following folder: /user/hive/warehouse/computersalesdb.db/sales_staging. Quickly confirm this.","dateUpdated":"2016-11-18T18:01:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177996_1783128006","id":"20161114-204239_1883221198","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You can now assume that the new sales_staging table directory is on HDFS in the following folder: /user/hive/warehouse/computersalesdb.db/sales_staging. Quickly confirm this.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:330","dateFinished":"2016-11-18T18:01:00+0000","dateStarted":"2016-11-18T18:01:00+0000","focus":true},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db\"'","dateUpdated":"2016-11-15T20:52:35+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177996_1783128006","id":"20161101-232053_1227459589","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:52:35+0000","dateFinished":"2016-11-15T20:52:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:331","errorMessage":""},{"text":"%md\nSure enough, the sales_staging directory was created and is now being managed by Hive.\n\nAsk Hive to show you the tables in the computersalesdb database. Confirm your new sales_staging table is in the Hive catalog.\n","dateUpdated":"2016-11-18T18:01:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177996_1783128006","id":"20161114-204528_925151006","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Sure enough, the sales_staging directory was created and is now being managed by Hive.</p>\n<p>Ask Hive to show you the tables in the computersalesdb database. Confirm your new sales_staging table is in the Hive catalog.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:332","dateFinished":"2016-11-18T18:01:04+0000","dateStarted":"2016-11-18T18:01:04+0000","focus":true},{"text":"%jdbc(hive)\nSHOW TABLES in computersalesdb","dateUpdated":"2016-11-15T20:52:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tab_name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"tab_name","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177997_1782743257","id":"20161101-232115_1530776116","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:52:42+0000","dateFinished":"2016-11-15T20:52:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:333","errorMessage":""},{"title":"Managed Partitioned Tables","text":"%md\nNow you will create a partitioned table. This table will be a managed table – Hive will manage the metadata and lifecycle of this table, just like the tables we previously created.\n\nNow, create the sales table. This table will be partitioned on the sales date.\n","dateUpdated":"2016-11-18T18:01:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177997_1782743257","id":"20161114-210120_515625299","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now you will create a partitioned table. This table will be a managed table – Hive will manage the metadata and lifecycle of this table, just like the tables we previously created.</p>\n<p>Now, create the sales table. This table will be partitioned on the sales date.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:334","dateFinished":"2016-11-18T18:01:08+0000","dateStarted":"2016-11-18T18:01:08+0000","focus":true},{"text":"%jdbc(hive)\nCREATE TABLE computersalesdb.sales\n (\t\n  \t\tcust_id    STRING,\n \t \tprod_num   STRING,\n  \t\tqty        INT,\n  \t\tsales_id   STRING\n )\n COMMENT 'Table for analysis of sales data'\n PARTITIONED BY (sales_date STRING)\n ROW FORMAT DELIMITED\n   \t\tFIELDS TERMINATED BY ','\n STORED AS TEXTFILE\n","dateUpdated":"2016-11-16T17:45:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177997_1782743257","id":"20161101-232340_931669456","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-16T17:45:50+0000","dateFinished":"2016-11-16T17:45:51+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:335","errorMessage":""},{"text":"%md\nNotice that you listed the sales_date in the PARTITIONED BY clause instead of listing it in the data column metadata. \nSince you are partitioning on sales_date, Hive will keep track of the dates for you outside of the actual data.\n\nLet’s view the extended details of our new table.","dateUpdated":"2016-11-18T18:01:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177997_1782743257","id":"20161114-210339_249877831","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notice that you listed the sales_date in the PARTITIONED BY clause instead of listing it in the data column metadata.\n<br  />Since you are partitioning on sales_date, Hive will keep track of the dates for you outside of the actual data.</p>\n<p>Let’s view the extended details of our new table.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:336","dateFinished":"2016-11-18T18:01:13+0000","dateStarted":"2016-11-18T18:01:13+0000","focus":true},{"text":"%jdbc(hive)\nDESCRIBE EXTENDED computersalesdb.sales","dateUpdated":"2016-11-15T20:52:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177998_1783897504","id":"20161101-232541_1969478377","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:52:56+0000","dateFinished":"2016-11-15T20:52:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:337","errorMessage":""},{"text":"%md\nYou can see a directory was created on HDFS at: /user/hive/warehouse/computersalesdb.db/sales.\n\nWhen you later put data into this table, a new directory will be created inside the sales directory for EACH partition.\n\nThe following line in the details shows us how our table is partitioned: partitionKeys: [FieldSchema(name:sales_date, type:string, comment:null)]\n","dateUpdated":"2016-11-18T18:01:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177998_1783897504","id":"20161114-210638_2096577601","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You can see a directory was created on HDFS at: /user/hive/warehouse/computersalesdb.db/sales.</p>\n<p>When you later put data into this table, a new directory will be created inside the sales directory for EACH partition.</p>\n<p>The following line in the details shows us how our table is partitioned: partitionKeys: [FieldSchema(name:sales_date, type:string, comment:null)]</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:338","dateFinished":"2016-11-18T18:01:22+0000","dateStarted":"2016-11-18T18:01:22+0000","focus":true},{"title":"External Table","text":"%md\nAnother department in our fictitious computer company would like to be able to analyze the customer data. It therefore makes sense that we setup the customer table as EXTERNAL so they can use their tools on the data and we can use ours (Hive). We will place a copy of the Customer.csv file in HDFS and then create a new table in Hive that points to this data.\n\nFirst you need to create a new directory – let’s call it “shared_hive_data” - on HDFS that can house our Customer.csv data file. Let’s put this in the /tmp directory on HDFS. Run the command to make the new directory on HDFS.","dateUpdated":"2016-11-18T18:01:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177998_1783897504","id":"20161114-210813_1829322524","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Another department in our fictitious computer company would like to be able to analyze the customer data. It therefore makes sense that we setup the customer table as EXTERNAL so they can use their tools on the data and we can use ours (Hive). We will place a copy of the Customer.csv file in HDFS and then create a new table in Hive that points to this data.</p>\n<p>First you need to create a new directory – let’s call it “shared_hive_data” - on HDFS that can house our Customer.csv data file. Let’s put this in the /tmp directory on HDFS. Run the command to make the new directory on HDFS.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:339","dateFinished":"2016-11-18T18:01:29+0000","dateStarted":"2016-11-18T18:01:29+0000","focus":true},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -mkdir /tmp/shared_hive_data/\"'","dateUpdated":"2016-11-15T20:53:11+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177998_1783897504","id":"20161101-235840_1627214868","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:53:12+0000","dateFinished":"2016-11-15T20:53:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:340","errorMessage":""},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /tmp\"'","dateUpdated":"2016-11-15T20:53:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177998_1783897504","id":"20161114-211117_1458375305","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:53:19+0000","dateFinished":"2016-11-15T20:53:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:341","errorMessage":""},{"text":"%sh\nssh root@hive 'su - root bash -c \"hdfs dfs -chmod 777 /tmp/shared_hive_data\"'","dateUpdated":"2016-11-15T20:53:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177999_1783512755","id":"20161114-230157_1799314768","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:53:35+0000","dateFinished":"2016-11-15T20:53:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:342","errorMessage":""},{"text":"%md\nAbove, you will notice that /tmp/shared_hive_data was in fact created on HDFS. You also succesfully chmod'ed the directory.\n\nNow you will move a copy of the Customer.csv file that is currently sitting in /tmp/hive_data/ directory on the local Linux file system on the namenode container, into the /tmp/shared_hive_data directory on HDFS.","dateUpdated":"2016-11-18T18:01:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177999_1783512755","id":"20161114-211155_1891698359","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Above, you will notice that /tmp/shared_hive_data was in fact created on HDFS. You also succesfully chmod'ed the directory.</p>\n<p>Now you will move a copy of the Customer.csv file that is currently sitting in /tmp/hive_data/ directory on the local Linux file system on the namenode container, into the /tmp/shared_hive_data directory on HDFS.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:343","dateFinished":"2016-11-18T18:01:35+0000","dateStarted":"2016-11-18T18:01:35+0000","focus":true},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -put /tmp/hive_data/Customer.csv /tmp/shared_hive_data/Customer.csv\"'","dateUpdated":"2016-11-15T20:53:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177999_1783512755","id":"20161101-233204_908317387","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:53:45+0000","dateFinished":"2016-11-15T20:53:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:344","errorMessage":""},{"text":"%md\nConfirm that Customer.csv has been copied successfully into HDFS.","dateUpdated":"2016-11-18T18:01:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177999_1783512755","id":"20161115-174907_747895635","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Confirm that Customer.csv has been copied successfully into HDFS.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:345","dateFinished":"2016-11-18T18:01:39+0000","dateStarted":"2016-11-18T18:01:39+0000","focus":true},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /tmp/shared_hive_data\"'","dateUpdated":"2016-11-15T20:53:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177999_1783512755","id":"20161102-192052_450194886","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:53:59+0000","dateFinished":"2016-11-15T20:54:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:346","errorMessage":""},{"text":"%md\nRun the “cat” command to verify the data is in the Customer.csv file on HDFS.","dateUpdated":"2016-11-18T18:01:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177999_1783512755","id":"20161115-174946_2036624580","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Run the “cat” command to verify the data is in the Customer.csv file on HDFS.</p>\n"},"dateCreated":"2016-11-15T20:36:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:347","dateFinished":"2016-11-18T18:01:43+0000","dateStarted":"2016-11-18T18:01:43+0000","focus":true},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -cat /tmp/shared_hive_data/Customer.csv\"'","dateUpdated":"2016-11-15T20:54:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242177999_1783512755","id":"20161102-192125_1560200308","dateCreated":"2016-11-15T20:36:17+0000","dateStarted":"2016-11-15T20:54:21+0000","dateFinished":"2016-11-15T20:54:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:348","errorMessage":""},{"text":"%md\nNow you simply need to define the external customer table.","dateUpdated":"2016-11-18T18:01:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178000_1793900975","id":"20161115-175433_1448485861","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now you simply need to define the external customer table.</p>\n"},"dateCreated":"2016-11-15T20:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:349","dateFinished":"2016-11-18T18:01:48+0000","dateStarted":"2016-11-18T18:01:48+0000","focus":true},{"text":"%jdbc(hive)\nCREATE EXTERNAL TABLE IF NOT EXISTS computersalesdb.customer\n(\t\n  \t\tfname        STRING,\n \t \tlname        STRING,\n  \t\tstatus       STRING,\n  \t\ttelno        STRING,\n  \t\tcustomer_id  STRING,\n        city_zip     STRUCT<city:STRING, zip:STRING>\n )\n COMMENT 'External table for customer data'\n ROW FORMAT DELIMITED\n   \t \tFIELDS TERMINATED BY ','\n\t \tCOLLECTION ITEMS TERMINATED BY '|'\n LOCATION '/tmp/shared_hive_data'\n TBLPROPERTIES(\"skip.header.line.count\"=\"1\")\n","dateUpdated":"2016-11-15T20:54:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178000_1793900975","id":"20161101-234255_634891378","dateCreated":"2016-11-15T20:36:18+0000","dateStarted":"2016-11-15T20:54:34+0000","dateFinished":"2016-11-15T20:54:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:350","errorMessage":""},{"text":"%md\nThere are a few things to note here. First, you used the EXTERNAL keyword in the CREATE line. \nNotice that the “stored as line” is left out when you created this external table since the default format is already set to TEXTFILE. You also added the LOCATION ‘location/of/datadirectory’ line to the end of the statement.\n\nHive expects that LOCATION will be a directory, not a file. For our exercises we will only have a single file in the shared_hive_data directory. However, you could put multiple customer data files into the shared_hive_data directory and Hive would use them all for your EXTERNAL table! That is a common scenario.\n","dateUpdated":"2016-11-18T18:01:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178000_1793900975","id":"20161115-175626_1513640643","result":{"code":"SUCCESS","type":"HTML","msg":"<p>There are a few things to note here. First, you used the EXTERNAL keyword in the CREATE line.\n<br  />Notice that the “stored as line” is left out when you created this external table since the default format is already set to TEXTFILE. You also added the LOCATION ‘location/of/datadirectory’ line to the end of the statement.</p>\n<p>Hive expects that LOCATION will be a directory, not a file. For our exercises we will only have a single file in the shared_hive_data directory. However, you could put multiple customer data files into the shared_hive_data directory and Hive would use them all for your EXTERNAL table! That is a common scenario.</p>\n"},"dateCreated":"2016-11-15T20:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:351","dateFinished":"2016-11-18T18:01:54+0000","dateStarted":"2016-11-18T18:01:54+0000","focus":true},{"text":"%md\nLet’s view the extended details of the new external customer table.","dateUpdated":"2016-11-18T18:01:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178000_1793900975","id":"20161115-175806_81648482","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let’s view the extended details of the new external customer table.</p>\n"},"dateCreated":"2016-11-15T20:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:352","dateFinished":"2016-11-18T18:01:58+0000","dateStarted":"2016-11-18T18:01:58+0000","focus":true},{"text":"%jdbc(hive)\nDESCRIBE EXTENDED computersalesdb.customer","dateUpdated":"2016-11-15T20:54:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"col_name","index":0,"aggr":"sum"}],"values":[{"name":"data_type","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"col_name","index":0,"aggr":"sum"},"yAxis":{"name":"data_type","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178000_1793900975","id":"20161101-234636_87946811","dateCreated":"2016-11-15T20:36:18+0000","dateStarted":"2016-11-15T20:54:41+0000","dateFinished":"2016-11-15T20:54:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:353","errorMessage":""},{"text":"%md\nYou can see above that the location points to the /tmp/shared_hive_data directory that you designated on HDFS. Also notice towards the end of the output that tableType:EXTERNAL_TABLE.","dateUpdated":"2016-11-18T18:02:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178001_1793516226","id":"20161115-175849_1353279268","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You can see above that the location points to the /tmp/shared_hive_data directory that you designated on HDFS. Also notice towards the end of the output that tableType:EXTERNAL_TABLE.</p>\n"},"dateCreated":"2016-11-15T20:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:354","dateFinished":"2016-11-18T18:02:02+0000","dateStarted":"2016-11-18T18:02:02+0000","focus":true},{"text":"%md\nAsk Hive to show you the tables in the metastore. Then check the Hive warehouse on HDFS to prove to yourself that Hive did NOT create a customer directory within it's default warehouse location. \n\nHive will manage the metadata for the external table, but it wont manage the actual data.","dateUpdated":"2016-11-18T18:02:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178001_1793516226","id":"20161115-175946_1164353840","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Ask Hive to show you the tables in the metastore. Then check the Hive warehouse on HDFS to prove to yourself that Hive did NOT create a customer directory within it's default warehouse location.</p>\n<p>Hive will manage the metadata for the external table, but it wont manage the actual data.</p>\n"},"dateCreated":"2016-11-15T20:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:355","dateFinished":"2016-11-18T18:02:05+0000","dateStarted":"2016-11-18T18:02:05+0000","focus":true},{"text":"%jdbc(hive)\nshow tables in computersalesdb","dateUpdated":"2016-11-15T20:54:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tab_name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"tab_name","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178001_1793516226","id":"20161103-173827_614815212","dateCreated":"2016-11-15T20:36:18+0000","dateStarted":"2016-11-15T20:54:56+0000","dateFinished":"2016-11-15T20:54:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:356","errorMessage":""},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db\"'","dateUpdated":"2016-11-15T20:55:10+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178001_1793516226","id":"20161102-200136_372174387","dateCreated":"2016-11-15T20:36:18+0000","dateStarted":"2016-11-15T20:55:10+0000","dateFinished":"2016-11-15T20:55:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:357","errorMessage":""},{"text":"%md\nSince <b>customer</b> is an external table and Hive already knows where the data is sitting, you can already begin to run queries on this table.\n\nReward yourself for completing this lab by running a simple select query as proof.","dateUpdated":"2016-11-18T18:02:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178002_1794670473","id":"20161115-180218_304770031","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Since <b>customer</b> is an external table and Hive already knows where the data is sitting, you can already begin to run queries on this table.</p>\n<p>Reward yourself for completing this lab by running a simple select query as proof.</p>\n"},"dateCreated":"2016-11-15T20:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:358","dateFinished":"2016-11-18T18:02:10+0000","dateStarted":"2016-11-18T18:02:10+0000","focus":true},{"text":"%jdbc(hive)\nSELECT * FROM computersalesdb.customer","dateUpdated":"2016-11-15T20:55:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"customer.fname","index":0,"aggr":"sum"}],"values":[{"name":"customer.lname","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"customer.fname","index":0,"aggr":"sum"},"yAxis":{"name":"customer.lname","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178002_1794670473","id":"20161101-234716_475481334","dateCreated":"2016-11-15T20:36:18+0000","dateStarted":"2016-11-15T20:55:19+0000","dateFinished":"2016-11-15T20:55:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:359","errorMessage":""},{"text":"%md\nThis Hive query didn’t run any MapReduce jobs. Hive is able to read the data file and return the results without using MapReduce, since this is a simple SELECT and LIMIT statement.","dateUpdated":"2016-11-18T18:02:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178002_1794670473","id":"20161102-192039_2096154804","result":{"code":"SUCCESS","type":"HTML","msg":"<p>This Hive query didn’t run any MapReduce jobs. Hive is able to read the data file and return the results without using MapReduce, since this is a simple SELECT and LIMIT statement.</p>\n"},"dateCreated":"2016-11-15T20:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:360","dateFinished":"2016-11-18T18:02:14+0000","dateStarted":"2016-11-18T18:02:14+0000","focus":true},{"text":"%md\n\nCongratulations! You now know how to create, alter and remove databases in Hive. You can create managed, external, and partitioned Hive tables. \nYou also are familiar with the sample data that will be used in this course. You may move on to the next Unit. ","dateUpdated":"2016-11-18T18:02:20+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479242178002_1794670473","id":"20161115-180406_255194290","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Congratulations! You now know how to create, alter and remove databases in Hive. You can create managed, external, and partitioned Hive tables.\n<br  />You also are familiar with the sample data that will be used in this course. You may move on to the next Unit.</p>\n"},"dateCreated":"2016-11-15T20:36:18+0000","dateStarted":"2016-11-18T18:02:20+0000","dateFinished":"2016-11-18T18:02:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:361","focus":true},{"text":"","dateUpdated":"2016-11-16T18:22:28+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479320536950_337554718","id":"20161116-182216_1000253281","dateCreated":"2016-11-16T18:22:16+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:362","errorMessage":""}],"name":"Hive Lab 2","id":"2C1Z8MH3K","angularObjects":{"2C1FSWPV5:shared_process":[],"2C2ANMC3H:shared_process":[],"2C27C2CAW:shared_process":[],"2C1EQ4DYT:shared_process":[],"2C2D85D6Q:shared_process":[],"2BYMYHYE7:shared_process":[],"2BZJ3HWCJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}